{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Job Market and Salaries Analysis \n",
    "\n",
    "For our final project, we have selected the Stack Overflow Developer Survey dataset, \n",
    "which contains detailed responses from developers regarding their job roles, skills, \n",
    "technologies used, and salary information. This dataset is particularly relevant to the \n",
    "tech industry, which is a major focus of our group, and will provide insights into the tech \n",
    "job market by collecting responses from developers worldwide. It covers various topics \n",
    "such as job roles, salary, coding activities, education, technology usage, and job \n",
    "satisfaction.<br>\n",
    "\n",
    "Team Eyy<br>\n",
    "Members:  \n",
    "- Julianne Kristine D. Aban \n",
    "- Derich Andre G. Arcilla \n",
    "- Jennifer Bendoy \n",
    "- Richelle Ann C. Candidato \n",
    "- Marc Francis B. Gomolon \n",
    "- Phoebe Kae A. Plasus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING DATA SET & LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "# df = pd.read_csv('survey_results_filtered.csv')\n",
    "df = pd.read_csv('survey_results_public.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand display settings to show all columns\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 200)     # Adjust rows if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information: name, number of missing values, and dtype\n",
    "column_info = pd.DataFrame({\n",
    "    'Column Name': df.columns,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Data Type': df.dtypes\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Print the column information\n",
    "print(column_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Filter columns with more than 50% missing values\n",
    "high_missing_cols = missing_percentage[missing_percentage > 50]\n",
    "print(\"Columns with more than 50% missing values:\")\n",
    "print(high_missing_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 50% missing values\n",
    "df_cleaned = df.drop(columns=high_missing_cols.index)\n",
    "print(f\"Dataset shape after dropping columns: {df_cleaned.shape}\")\n",
    "\n",
    "# Show the names of the remaining columns\n",
    "remaining_columns = df_cleaned.columns\n",
    "print(f\"Remaining columns ({len(remaining_columns)}):\")\n",
    "print(remaining_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numerical values with median\n",
    "numerical_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_cleaned[numerical_cols] = df_cleaned[numerical_cols].fillna(df_cleaned[numerical_cols].median())\n",
    "\n",
    "# Fill missing categorical values with mode\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "df_cleaned[categorical_cols] = df_cleaned[categorical_cols].fillna(df_cleaned[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Check for missing values in numerical columns\n",
    "print(\"Missing values in numerical columns:\")\n",
    "print(df_cleaned[numerical_cols].isnull().sum())\n",
    "\n",
    "# Check for missing values in categorical columns\n",
    "print(\"Missing values in categorical columns:\")\n",
    "print(df_cleaned[categorical_cols].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Cleaned File\n",
    "df_cleaned.to_csv('cleaned_survey_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df_cleaned = pd.read_csv('cleaned_survey_results.csv')\n",
    "\n",
    "# Define the columns to process (update based on your data)\n",
    "columns_to_encode = [\n",
    "    'LanguageHaveWorkedWith',\n",
    "    'DatabaseHaveWorkedWith',\n",
    "    'WebframeHaveWorkedWith',\n",
    "    'ToolsTechHaveWorkedWith',\n",
    "    'DevType'\n",
    "]\n",
    "\n",
    "# Create a binary matrix\n",
    "binary_df = pd.DataFrame()\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    if col in df_cleaned.columns:\n",
    "        # Split semi-colon-separated values and create a binary matrix\n",
    "        split_data = df_cleaned[col].str.get_dummies(sep=';')\n",
    "        binary_df = pd.concat([binary_df, split_data], axis=1)\n",
    "\n",
    "# Convert the binary matrix to bool type\n",
    "binary_df_bool = binary_df.astype(bool)\n",
    "\n",
    "# Apply the Apriori algorithm using the bool DataFrame\n",
    "frequent_itemsets = apriori(binary_df_bool, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Calculate the total number of itemsets\n",
    "num_itemsets = len(frequent_itemsets)\n",
    "\n",
    "# Generate association rules, including the 'num_itemsets' parameter\n",
    "rules = association_rules(frequent_itemsets, num_itemsets=num_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# This code identifies patterns in how developers use technologies like programming languages, databases, and frameworks. \n",
    "# By applying the Apriori algorithm, it reveals frequent combinations (e.g., Python with SQL) and strong associations between tools, \n",
    "# helping understand how technologies are commonly grouped in real-world usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and display the top rules\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "top_rules = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)\n",
    "\n",
    "# Conditional formatting with darker gold and softer pink, and black text for gold background\n",
    "def highlight_strong_tool_associations(val):\n",
    "    color = ''\n",
    "    text_color = 'color: black'  # Default text color for gold background\n",
    "    if isinstance(val, (int, float)):  # Apply formatting only to numerical values\n",
    "        if val > 0.7:  # For high values\n",
    "            color = 'background-color: #D4AF37'  # Darker gold for high values\n",
    "            text_color = 'color: black'  # Ensure text is black on gold\n",
    "        elif val < 0.3:  # For low values\n",
    "            color = 'background-color: maroon'  # Softer pink for low values\n",
    "    return color + ';' + text_color\n",
    "\n",
    "# Apply conditional formatting to the table\n",
    "styled_table = top_rules.style.applymap(highlight_strong_tool_associations, subset=['support', 'confidence', 'lift'])\n",
    "\n",
    "# Display the styled table\n",
    "styled_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to analyze\n",
    "employment_columns = ['Employment', 'RemoteWork', 'OrgSize']\n",
    "tech_columns = [\n",
    "    'LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith',\n",
    "    'WebframeHaveWorkedWith', 'ToolsTechHaveWorkedWith'\n",
    "]\n",
    "\n",
    "# Convert binary-encoded dataframes to boolean\n",
    "binary_employment = pd.get_dummies(df_cleaned[employment_columns], prefix=employment_columns).astype(bool)\n",
    "binary_tech = pd.DataFrame()\n",
    "\n",
    "for col in tech_columns:\n",
    "    if col in df_cleaned.columns:\n",
    "        split_data = df_cleaned[col].str.get_dummies(sep=';').astype(bool)\n",
    "        binary_tech = pd.concat([binary_tech, split_data], axis=1)\n",
    "\n",
    "\n",
    "# Combine employment and tech binary data\n",
    "binary_data = pd.concat([binary_employment, binary_tech], axis=1)\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "frequent_itemsets = apriori(binary_data, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, num_itemsets=num_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Filter and sort the rules\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "print(\"Top 10 association rules for Employment and Technology:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "\n",
    "\n",
    "# This code explores relationships between employment factors (e.g., job type, remote work) and technology preferences. \n",
    "# It highlights how professional roles influence technology choices, such as remote workers preferring tools like Docker, \n",
    "# offering insights into technology trends based on work environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and sort the rules\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "top_rules = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)\n",
    "\n",
    "# Conditional formatting with darker gold and maroon, and black text for gold background\n",
    "def highlight_employment_technology_associations(val):\n",
    "    color = ''\n",
    "    text_color = 'color: black'  # Default text color for gold background\n",
    "    if isinstance(val, (int, float)):  # Apply formatting only to numerical values\n",
    "        if val > 0.7:  # For high values\n",
    "            color = 'background-color: #D4AF37'  # Darker gold for high values\n",
    "            text_color = 'color: black'  # Ensure text is black on gold\n",
    "        elif val < 0.3:  # For low values\n",
    "            color = 'background-color: maroon'  # Softer maroon for low values\n",
    "    return color + ';' + text_color\n",
    "\n",
    "# Apply conditional formatting to the table\n",
    "styled_table = top_rules.style.applymap(highlight_employment_technology_associations, subset=['support', 'confidence', 'lift'])\n",
    "\n",
    "# Display the styled table\n",
    "styled_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
