{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Job Market and Salaries Analysis \n",
    "\n",
    "For our final project, we have selected the Stack Overflow Developer Survey dataset, \n",
    "which contains detailed responses from developers regarding their job roles, skills, \n",
    "technologies used, and salary information. This dataset is particularly relevant to the \n",
    "tech industry, which is a major focus of our group, and will provide insights into the tech \n",
    "job market by collecting responses from developers worldwide. It covers various topics \n",
    "such as job roles, salary, coding activities, education, technology usage, and job \n",
    "satisfaction.<br>\n",
    "\n",
    "Team Eyy<br>\n",
    "Members:  \n",
    "- Julianne Kristine D. Aban \n",
    "- Derich Andre G. Arcilla \n",
    "- Jennifer Bendoy \n",
    "- Richelle Ann C. Candidato \n",
    "- Marc Francis B. Gomolon \n",
    "- Phoebe Kae A. Plasus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Analysis Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "# File path to the cleaned survey data\n",
    "file_path = 'cleaned_survey_results.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    " \n",
    "# Columns to drop based on irrelevance or redundancy\n",
    "columns_to_drop = ['ResponseId', 'Unnamed: 17', 'Currency']  # Excluded JobSat and Salary from this list\n",
    "data_cleaned = data.drop(columns=columns_to_drop, errors='ignore')\n",
    " \n",
    "# Selecting relevant columns for clustering\n",
    "selected_columns = ['EdLevel', 'YearsCode', 'YearsCodePro']\n",
    "data_numeric = data_cleaned[selected_columns]\n",
    " \n",
    "# Handling missing values using mean imputation for numerical columns\n",
    "imputer = SimpleImputer(strategy='most_frequent')  # Use 'most_frequent' for categorical data\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data_numeric), columns=data_numeric.columns)\n",
    " \n",
    "# Converting categorical columns to numerical using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in ['EdLevel', 'YearsCode', 'YearsCodePro']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data_imputed[column] = label_encoders[column].fit_transform(data_imputed[column])\n",
    " \n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_imputed)\n",
    " \n",
    "# Finding the optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "range_k = range(1, 11)\n",
    "for k in range_k:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    " \n",
    "# Plotting the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range_k, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    " \n",
    "# Applying K-Means with the chosen number of clusters\n",
    "optimal_k = 4  # Adjust based on elbow curve results\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_scaled)\n",
    " \n",
    "# Adding cluster labels to the dataset\n",
    "data_imputed['Cluster'] = clusters\n",
    " \n",
    "# Visualizing the clusters using a pair plot\n",
    "data_imputed['Cluster'] = data_imputed['Cluster'].astype(str)  # Convert cluster labels to strings for visualization\n",
    "sns.pairplot(data_imputed, hue='Cluster', diag_kind='kde', corner=True)\n",
    "plt.show()\n",
    " \n",
    "# Saving the clustered dataset to a new CSV file\n",
    "data_imputed.to_csv('clustered_survey_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = pd.read_csv('cleaned_file.csv')\n",
    "\n",
    "print(\"Missing values in each column:\")\n",
    "print(df[['YearsCodePro', 'YearsCode', 'EdLevel', 'JobSat', 'DevType', 'Salary']].isnull().sum())\n",
    "\n",
    "df = df.dropna(subset=['YearsCodePro', 'YearsCode', 'EdLevel', 'JobSat', 'DevType', 'Salary'])\n",
    "\n",
    "print(\"\\nColumns in the DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "if 'Salary' not in df.columns:\n",
    "    print(\"The column 'Salary' is not available in the DataFrame. Please check the column names.\")\n",
    "else:\n",
    "    X = df[['YearsCodePro', 'YearsCode', 'EdLevel', 'JobSat', 'DevType']]  \n",
    "    y = df['Salary']  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[ \n",
    "            ('num', 'passthrough', ['YearsCodePro', 'YearsCode', 'JobSat']),  \n",
    "            ('cat', OneHotEncoder(), ['EdLevel', 'DevType'])  \n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('model', LinearRegression())])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'\\nMean Squared Error: {mse}')\n",
    "    print(f'R-squared: {r2}')\n",
    "\n",
    "    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    coefficients = pipeline.named_steps['model'].coef_\n",
    "\n",
    "    feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "    print(\"\\nFeature Importance (Coefficients):\")\n",
    "    print(feature_importance.sort_values(by='Coefficient', ascending=False))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.xlabel('Actual Salaries')\n",
    "    plt.ylabel('Predicted Salaries')\n",
    "    plt.title('Actual vs Predicted Salaries')\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #erase if being called already\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df_cleaned = pd.read_csv('cleaned_survey_results.csv')\n",
    "\n",
    "\n",
    "# Apriori Algorithm 1\n",
    "\n",
    "# Define the columns to process (update based on your data)\n",
    "columns_to_encode = [\n",
    "    'LanguageHaveWorkedWith',\n",
    "    'DatabaseHaveWorkedWith',\n",
    "    'WebframeHaveWorkedWith',\n",
    "    'ToolsTechHaveWorkedWith',\n",
    "    'DevType'\n",
    "]\n",
    "\n",
    "# Create a binary matrix\n",
    "binary_df = pd.DataFrame()\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    if col in df_cleaned.columns:\n",
    "        # Split semi-colon-separated values and create a binary matrix\n",
    "        split_data = df_cleaned[col].str.get_dummies(sep=';')\n",
    "        binary_df = pd.concat([binary_df, split_data], axis=1)\n",
    "\n",
    "# Convert the binary matrix to bool type\n",
    "binary_df_bool = binary_df.astype(bool)\n",
    "\n",
    "# Apply the Apriori algorithm using the bool DataFrame\n",
    "frequent_itemsets = apriori(binary_df_bool, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Calculate the total number of itemsets\n",
    "num_itemsets = len(frequent_itemsets)\n",
    "\n",
    "# Generate association rules, including the 'num_itemsets' parameter\n",
    "rules = association_rules(frequent_itemsets, num_itemsets=num_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Sort and display the top rules\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "print(\"Top 10 association rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "\n",
    "\n",
    "# This code identifies patterns in how developers use technologies like programming languages, databases, and frameworks. \n",
    "# By applying the Apriori algorithm, it reveals frequent combinations (e.g., Python with SQL) and strong associations between tools, \n",
    "# helping understand how technologies are commonly grouped in real-world usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori Algorithm 2\n",
    "\n",
    "# Columns to analyze\n",
    "employment_columns = ['Employment', 'RemoteWork', 'OrgSize']\n",
    "tech_columns = [\n",
    "    'LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith',\n",
    "    'WebframeHaveWorkedWith', 'ToolsTechHaveWorkedWith'\n",
    "]\n",
    "\n",
    "# Convert binary-encoded dataframes to boolean\n",
    "binary_employment = pd.get_dummies(df_cleaned[employment_columns], prefix=employment_columns).astype(bool)\n",
    "binary_tech = pd.DataFrame()\n",
    "\n",
    "for col in tech_columns:\n",
    "    if col in df_cleaned.columns:\n",
    "        split_data = df_cleaned[col].str.get_dummies(sep=';').astype(bool)\n",
    "        binary_tech = pd.concat([binary_tech, split_data], axis=1)\n",
    "\n",
    "\n",
    "# Combine employment and tech binary data\n",
    "binary_data = pd.concat([binary_employment, binary_tech], axis=1)\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "frequent_itemsets = apriori(binary_data, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, num_itemsets=num_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Filter and sort the rules\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "print(\"Top 10 association rules for Employment and Technology:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "\n",
    "\n",
    "# This code explores relationships between employment factors (e.g., job type, remote work) and technology preferences. \n",
    "# It highlights how professional roles influence technology choices, such as remote workers preferring tools like Docker, \n",
    "# offering insights into technology trends based on work environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
