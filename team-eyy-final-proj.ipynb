{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Job Market and Salaries Analysis \n",
    "\n",
    "For our final project, we have selected the Stack Overflow Developer Survey dataset, \n",
    "which contains detailed responses from developers regarding their job roles, skills, \n",
    "technologies used, and salary information. This dataset is particularly relevant to the \n",
    "tech industry, which is a major focus of our group, and will provide insights into the tech \n",
    "job market by collecting responses from developers worldwide. It covers various topics \n",
    "such as job roles, salary, coding activities, education, technology usage, and job \n",
    "satisfaction.<br>\n",
    "\n",
    "Team Eyy<br>\n",
    "Members:  \n",
    "- Julianne Kristine D. Aban \n",
    "- Derich Andre G. Arcilla \n",
    "- Jennifer Bendoy \n",
    "- Richelle Ann C. Candidato \n",
    "- Marc Francis B. Gomolon \n",
    "- Phoebe Kae A. Plasus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df2 = pd.read_csv('cleaned_survey_results.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "df2.head()\n",
    "\n",
    "\n",
    "\n",
    "# Check if the 'Country' column exists, otherwise replace with the relevant column name\n",
    "if 'Country' in df2.columns:\n",
    "    country_counts = df2['Country'].value_counts().head(10)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=country_counts.index, y=country_counts.values, palette=\"Blues_d\")\n",
    "    plt.title('Top 10 Countries by Respondents', fontsize=16)\n",
    "    plt.xlabel('Country', fontsize=14)\n",
    "    plt.ylabel('Number of Respondents', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No 'Country' column found in the dataset.\")\n",
    "\n",
    "\n",
    "# Handle missing or invalid values for 'YearsCode' and 'YearsCodePro'\n",
    "df2 = df2.dropna(subset=['YearsCode', 'YearsCodePro'])\n",
    "\n",
    "# Convert 'YearsCode' and 'YearsCodePro' to numeric values\n",
    "df2['YearsCode'] = pd.to_numeric(df2['YearsCode'], errors='coerce')\n",
    "df2['YearsCodePro'] = pd.to_numeric(df2['YearsCodePro'], errors='coerce')\n",
    "\n",
    "# Drop rows where conversion failed\n",
    "df2 = df2.dropna(subset=['YearsCode', 'YearsCodePro'])\n",
    "\n",
    "# Set up the figure for multiple plots\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Distribution plot for YearsCode\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df2['YearsCode'], kde=True, color='skyblue', bins=20)\n",
    "plt.title('Distribution of Years of Coding Experience')\n",
    "plt.xlabel('Years of Coding Experience')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot 2: Distribution plot for YearsCodePro\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df2['YearsCodePro'], kde=True, color='salmon', bins=20)\n",
    "plt.title('Distribution of Professional Coding Experience')\n",
    "plt.xlabel('Years of Professional Coding Experience')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Age'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Age Distribution of Respondents', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Get the top 15 countries by respondent count\n",
    "top_countries = df['Country'].value_counts().head(15).index\n",
    "filtered_df = df[df['Country'].isin(top_countries)]\n",
    "\n",
    "# Plot the boxplot for top countries\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=filtered_df, x='Country', y='Age')\n",
    "plt.title('Age Distribution by Top 15 Countries', fontsize=16)\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Age', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Bar Chart for Salary by Education Level\n",
    "avg_salary_by_education = df.groupby('EdLevel')['CompTotal'].mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_salary_by_education.plot(kind='bar', color='orange')\n",
    "plt.title('Average Salary by Education Level', fontsize=16)\n",
    "plt.xlabel('Education Level', fontsize=12)\n",
    "plt.ylabel('Average Salary', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cleaning 'YearsCodePro'\n",
    "df['YearsCodePro'] = df['YearsCodePro'].replace({\n",
    "    'Less than 1 year': 0.5,\n",
    "    'More than 50 years': 50,\n",
    "    'NA': np.nan\n",
    "}).astype(float)  # Convert to numeric\n",
    "\n",
    "# Cleaning 'CompTotal'\n",
    "df['CompTotal'] = df['CompTotal'].replace('NA', np.nan)  # Replace 'NA' with NaN\n",
    "df['CompTotal'] = pd.to_numeric(df['CompTotal'], errors='coerce')  # Convert to numeric\n",
    "\n",
    "# Remove outliers in 'CompTotal' (e.g., >99th percentile)\n",
    "q99 = df['CompTotal'].quantile(0.99)  # 99th percentile\n",
    "df_cleaned = df[(df['CompTotal'] <= q99) & (df['CompTotal'] > 0)].dropna(subset=['YearsCodePro', 'CompTotal'])\n",
    "\n",
    "# Scatter plot with trend line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_cleaned, x='YearsCodePro', y='CompTotal', alpha=0.6)\n",
    "sns.regplot(data=df_cleaned, x='YearsCodePro', y='CompTotal', scatter=False, color='red')\n",
    "plt.title('CompTotal vs. Professional Years of Experience', fontsize=16)\n",
    "plt.xlabel('Professional Years of Experience', fontsize=12)\n",
    "plt.ylabel('Salary (CompTotal)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example DataFrame with Age, YearsCodePro, CompTotal, YearsCode columns\n",
    "data = {\n",
    "    \"Age\": [\"18-24 years old\", \"25-34 years old\", \"35-44 years old\", \"Prefer not to say\", \n",
    "            \"Under 18 years old\", \"65 years or older\"],\n",
    "    \"YearsCodePro\": [1, 5, 10, 3, 0, 30],\n",
    "    \"CompTotal\": [60000, 80000, 120000, 75000, 50000, 150000],\n",
    "    \"YearsCode\": [2, 6, 12, 4, 1, 35]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the mapping for Age\n",
    "age_mapping = {\n",
    "    '18-24 years old': 21,\n",
    "    '25-34 years old': 29.5,\n",
    "    '35-44 years old': 39.5,\n",
    "    '45-54 years old': 49.5,\n",
    "    '55-64 years old': 59.5,\n",
    "    '65 years or older': 70,\n",
    "    'Under 18 years old': 17,\n",
    "    'Prefer not to say': np.nan\n",
    "}\n",
    "\n",
    "# Replace Age column values with numeric equivalents\n",
    "df['Age'] = df['Age'].replace(age_mapping).astype(float)\n",
    "\n",
    "# Heatmap for Numeric Correlations\n",
    "numeric_cols = ['Age', 'YearsCodePro', 'CompTotal', 'YearsCode']\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix for CompTotal, Age, and Experience', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Analysis Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "# File path to the cleaned survey data\n",
    "file_path = 'cleaned_survey_results.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    " \n",
    "# Columns to drop based on irrelevance or redundancy\n",
    "columns_to_drop = ['ResponseId', 'Unnamed: 17', 'Currency']  # Excluded JobSat and Salary from this list\n",
    "data_cleaned = data.drop(columns=columns_to_drop, errors='ignore')\n",
    " \n",
    "# Selecting relevant columns for clustering\n",
    "selected_columns = ['EdLevel', 'YearsCode', 'YearsCodePro']\n",
    "data_numeric = data_cleaned[selected_columns]\n",
    " \n",
    "# Handling missing values using mean imputation for numerical columns\n",
    "imputer = SimpleImputer(strategy='most_frequent')  # Use 'most_frequent' for categorical data\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data_numeric), columns=data_numeric.columns)\n",
    " \n",
    "# Converting categorical columns to numerical using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in ['EdLevel', 'YearsCode', 'YearsCodePro']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data_imputed[column] = label_encoders[column].fit_transform(data_imputed[column])\n",
    " \n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_imputed)\n",
    " \n",
    "# Finding the optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "range_k = range(1, 11)\n",
    "for k in range_k:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    " \n",
    "# Plotting the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range_k, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    " \n",
    "# Applying K-Means with the chosen number of clusters\n",
    "optimal_k = 4  # Adjust based on elbow curve results\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_scaled)\n",
    " \n",
    "# Adding cluster labels to the dataset\n",
    "data_imputed['Cluster'] = clusters\n",
    " \n",
    "# Visualizing the clusters using a pair plot\n",
    "data_imputed['Cluster'] = data_imputed['Cluster'].astype(str)  # Convert cluster labels to strings for visualization\n",
    "sns.pairplot(data_imputed, hue='Cluster', diag_kind='kde', corner=True)\n",
    "plt.show()\n",
    " \n",
    "# Saving the clustered dataset to a new CSV file\n",
    "data_imputed.to_csv('clustered_survey_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = pd.read_csv('cleaned_file.csv')\n",
    "\n",
    "print(\"Missing values in each column:\")\n",
    "print(df[['YearsCodePro', 'YearsCode', 'EdLevel', 'JobSat', 'DevType', 'Salary']].isnull().sum())\n",
    "\n",
    "df = df.dropna(subset=['YearsCodePro', 'YearsCode', 'EdLevel', 'JobSat', 'DevType', 'Salary'])\n",
    "\n",
    "print(\"\\nColumns in the DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "if 'Salary' not in df.columns:\n",
    "    print(\"The column 'Salary' is not available in the DataFrame. Please check the column names.\")\n",
    "else:\n",
    "    X = df[['YearsCodePro', 'YearsCode', 'EdLevel', 'JobSat', 'DevType']]  \n",
    "    y = df['Salary']  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[ \n",
    "            ('num', 'passthrough', ['YearsCodePro', 'YearsCode', 'JobSat']),  \n",
    "            ('cat', OneHotEncoder(), ['EdLevel', 'DevType'])  \n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('model', LinearRegression())])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'\\nMean Squared Error: {mse}')\n",
    "    print(f'R-squared: {r2}')\n",
    "\n",
    "    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    coefficients = pipeline.named_steps['model'].coef_\n",
    "\n",
    "    feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "    print(\"\\nFeature Importance (Coefficients):\")\n",
    "    print(feature_importance.sort_values(by='Coefficient', ascending=False))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.xlabel('Actual Salaries')\n",
    "    plt.ylabel('Predicted Salaries')\n",
    "    plt.title('Actual vs Predicted Salaries')\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #erase if being called already\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df_cleaned = pd.read_csv('cleaned_survey_results.csv')\n",
    "\n",
    "\n",
    "# Apriori Algorithm 1\n",
    "\n",
    "# Define the columns to process (update based on your data)\n",
    "columns_to_encode = [\n",
    "    'LanguageHaveWorkedWith',\n",
    "    'DatabaseHaveWorkedWith',\n",
    "    'WebframeHaveWorkedWith',\n",
    "    'ToolsTechHaveWorkedWith',\n",
    "    'DevType'\n",
    "]\n",
    "\n",
    "# Create a binary matrix\n",
    "binary_df = pd.DataFrame()\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    if col in df_cleaned.columns:\n",
    "        # Split semi-colon-separated values and create a binary matrix\n",
    "        split_data = df_cleaned[col].str.get_dummies(sep=';')\n",
    "        binary_df = pd.concat([binary_df, split_data], axis=1)\n",
    "\n",
    "# Convert the binary matrix to bool type\n",
    "binary_df_bool = binary_df.astype(bool)\n",
    "\n",
    "# Apply the Apriori algorithm using the bool DataFrame\n",
    "frequent_itemsets = apriori(binary_df_bool, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Calculate the total number of itemsets\n",
    "num_itemsets = len(frequent_itemsets)\n",
    "\n",
    "# Generate association rules, including the 'num_itemsets' parameter\n",
    "rules = association_rules(frequent_itemsets, num_itemsets=num_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Sort and display the top rules\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "print(\"Top 10 association rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "\n",
    "\n",
    "# This code identifies patterns in how developers use technologies like programming languages, databases, and frameworks. \n",
    "# By applying the Apriori algorithm, it reveals frequent combinations (e.g., Python with SQL) and strong associations between tools, \n",
    "# helping understand how technologies are commonly grouped in real-world usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori Algorithm 2\n",
    "\n",
    "# Columns to analyze\n",
    "employment_columns = ['Employment', 'RemoteWork', 'OrgSize']\n",
    "tech_columns = [\n",
    "    'LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith',\n",
    "    'WebframeHaveWorkedWith', 'ToolsTechHaveWorkedWith'\n",
    "]\n",
    "\n",
    "# Convert binary-encoded dataframes to boolean\n",
    "binary_employment = pd.get_dummies(df_cleaned[employment_columns], prefix=employment_columns).astype(bool)\n",
    "binary_tech = pd.DataFrame()\n",
    "\n",
    "for col in tech_columns:\n",
    "    if col in df_cleaned.columns:\n",
    "        split_data = df_cleaned[col].str.get_dummies(sep=';').astype(bool)\n",
    "        binary_tech = pd.concat([binary_tech, split_data], axis=1)\n",
    "\n",
    "\n",
    "# Combine employment and tech binary data\n",
    "binary_data = pd.concat([binary_employment, binary_tech], axis=1)\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "frequent_itemsets = apriori(binary_data, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, num_itemsets=num_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Filter and sort the rules\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "print(\"Top 10 association rules for Employment and Technology:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "\n",
    "\n",
    "# This code explores relationships between employment factors (e.g., job type, remote work) and technology preferences. \n",
    "# It highlights how professional roles influence technology choices, such as remote workers preferring tools like Docker, \n",
    "# offering insights into technology trends based on work environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
