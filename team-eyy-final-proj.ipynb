{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Job Market and Salaries Analysis \n",
    "\n",
    "For our final project, we have selected the Stack Overflow Developer Survey dataset, \n",
    "which contains detailed responses from developers regarding their job roles, skills, \n",
    "technologies used, and salary information. This dataset is particularly relevant to the \n",
    "tech industry, which is a major focus of our group, and will provide insights into the tech \n",
    "job market by collecting responses from developers worldwide. It covers various topics \n",
    "such as job roles, salary, coding activities, education, technology usage, and job \n",
    "satisfaction.<br>\n",
    "\n",
    "Team Eyy<br>\n",
    "Members:  \n",
    "- Julianne Kristine D. Aban \n",
    "- Derich Andre G. Arcilla \n",
    "- Jennifer Bendoy \n",
    "- Richelle Ann C. Candidato \n",
    "- Marc Francis B. Gomolon \n",
    "- Phoebe Kae A. Plasus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING DATA SET & LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "# df = pd.read_csv('survey_results_filtered.csv')\n",
    "df = pd.read_csv('survey_results_public.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand display settings to show all columns\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 200)     # Adjust rows if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information: name, number of missing values, and dtype\n",
    "column_info = pd.DataFrame({\n",
    "    'Column Name': df.columns,\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Data Type': df.dtypes\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Print the column information\n",
    "print(column_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Filter columns with more than 50% missing values\n",
    "high_missing_cols = missing_percentage[missing_percentage > 50]\n",
    "print(\"Columns with more than 50% missing values:\")\n",
    "print(high_missing_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 50% missing values\n",
    "df_cleaned = df.drop(columns=high_missing_cols.index)\n",
    "print(f\"Dataset shape after dropping columns: {df_cleaned.shape}\")\n",
    "\n",
    "# Show the names of the remaining columns\n",
    "remaining_columns = df_cleaned.columns\n",
    "print(f\"Remaining columns ({len(remaining_columns)}):\")\n",
    "print(remaining_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numerical values with median\n",
    "numerical_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_cleaned[numerical_cols] = df_cleaned[numerical_cols].fillna(df_cleaned[numerical_cols].median())\n",
    "\n",
    "# Fill missing categorical values with mode\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "df_cleaned[categorical_cols] = df_cleaned[categorical_cols].fillna(df_cleaned[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Check for missing values in numerical columns\n",
    "print(\"Missing values in numerical columns:\")\n",
    "print(df_cleaned[numerical_cols].isnull().sum())\n",
    "\n",
    "# Check for missing values in categorical columns\n",
    "print(\"Missing values in categorical columns:\")\n",
    "print(df_cleaned[categorical_cols].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Cleaned File\n",
    "df_cleaned.to_csv('cleaned_survey_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING CODE OF EXPERIENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning Years of Experience YearsCode(overall coding experience) && YearsCodePro (coding experience as a professional)\n",
    "\n",
    "# # Step 1: Replace 'NA' strings with NaN\n",
    "# df.replace('NA', pd.NA, inplace=True)\n",
    "\n",
    "# # Step 2: Convert 'YearsCode' and 'YearsCodePro' columns to numeric, \n",
    "# # forcing errors to NaN for any other non-numeric values\n",
    "# df['YearsCode'] = pd.to_numeric(df['YearsCode'], errors='coerce')\n",
    "# df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n",
    "\n",
    "# # Step 3: Impute missing values with the mean of each column\n",
    "# df['YearsCode'] = df['YearsCode'].fillna(df['YearsCode'].mean())\n",
    "# df['YearsCodePro'] = df['YearsCodePro'].fillna(df['YearsCodePro'].mean())\n",
    "\n",
    "# # Round the results to whole numbers (integers)\n",
    "# df['YearsCode'] = df['YearsCode'].round().astype(int)\n",
    "# df['YearsCodePro'] = df['YearsCodePro'].round().astype(int)\n",
    "\n",
    "# # Print the cleaned DataFrame with the two columns\n",
    "# # print(df[['YearsCode', 'YearsCodePro']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING EDUCATION LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean the EdLevel column:\n",
    "# # - Remove text in parentheses (e.g., \"(e.g. American high school, etc.)\")\n",
    "# # - Strip any extra spaces\n",
    "# df['EdLevel'] = df['EdLevel'].apply(lambda x: re.sub(r'\\(.*\\)', '', str(x)).strip())\n",
    "\n",
    "# # Mapping dictionary for converting text to numeric values\n",
    "# edlevel_mapping = {\n",
    "#     'Primary/elementary school': 1,\n",
    "#     'Secondary school': 2,\n",
    "#     \"Bachelor's degree\": 3,\n",
    "#     'Associate degree': 4,\n",
    "#     \"Master's degree\": 5,\n",
    "#     'Professional degree': 6,\n",
    "#     'Some college/university study without earning a degree': 0,\n",
    "#     'Something else': 0\n",
    "# }\n",
    "\n",
    "# # Replace text with numeric values according to the mapping\n",
    "# df['EdLevel'] = df['EdLevel'].replace(edlevel_mapping)\n",
    "\n",
    "# # Convert the column to numeric values (in case there are still mixed types)\n",
    "# df['EdLevel'] = pd.to_numeric(df['EdLevel'], errors='coerce')\n",
    "\n",
    "# # Handle missing values (NaN) and replace with -1\n",
    "# df['EdLevel'] = df['EdLevel'].fillna(-1)\n",
    "\n",
    "# # Print only the cleaned 'EdLevel' column\n",
    "# # print(df['EdLevel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING ORGSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display unique values in OrgSize and their counts\n",
    "# print(\"Unique values in OrgSize before parsing:\")\n",
    "# print(df_cluster['OrgSize'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to parse OrgSize values\n",
    "# def parse_org_size(value):\n",
    "#     if pd.isnull(value):  # Handle missing values\n",
    "#         return np.nan\n",
    "#     value = str(value).lower()\n",
    "    \n",
    "#     # Handle ranges like \"100 to 499 employees\"\n",
    "#     if \"to\" in value:\n",
    "#         try:\n",
    "#             return int(value.split(\"to\")[0].replace(\",\", \"\").strip())\n",
    "#         except ValueError:\n",
    "#             return np.nan\n",
    "    \n",
    "#     # Handle \"10,000 or more employees\"\n",
    "#     elif \"or more\" in value:\n",
    "#         try:\n",
    "#             return int(value.split(\"or\")[0].replace(\",\", \"\").strip())\n",
    "#         except ValueError:\n",
    "#             return np.nan\n",
    "\n",
    "#     # Handle freelancer entries\n",
    "#     elif \"just me\" in value or \"freelancer\" in value:\n",
    "#         return 1  # Single individual\n",
    "\n",
    "#     # Handle unknown or NA\n",
    "#     elif \"i don't know\" in value or value in [\"na\"]:\n",
    "#         return np.nan\n",
    "\n",
    "#     # Default fallback for unexpected values\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# # Display original unique values in OrgSize\n",
    "# print(\"Unique values in OrgSize before parsing:\")\n",
    "# print(df_cluster['OrgSize'].value_counts(dropna=False))\n",
    "\n",
    "# # Apply parsing to OrgSize column\n",
    "# df_cluster['OrgSize'] = df_cluster['OrgSize'].map(parse_org_size)\n",
    "\n",
    "# # Fill missing values with the median\n",
    "# df_cluster['OrgSize'] = df_cluster['OrgSize'].fillna(df_cluster['OrgSize'].median())\n",
    "\n",
    "# # Display transformed OrgSize values after parsing\n",
    "# print(\"\\nUnique values in OrgSize after parsing:\")\n",
    "# print(df_cluster['OrgSize'].value_counts())\n",
    "\n",
    "# # Optional: Display final values after filling missing values\n",
    "# print(\"\\nUnique values in OrgSize after filling missing values:\")\n",
    "# print(df_cluster['OrgSize'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING LANGUAGE HAVE WORKED WITH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handle NaN values by filling them with an empty string or dropping rows with NaN values\n",
    "# df['LanguageHaveWorkedWith'] = df['LanguageHaveWorkedWith'].fillna('')\n",
    "\n",
    "# # Split the 'LanguageHaveWorkedWith' column by semicolon\n",
    "# df['Technologies'] = df['LanguageHaveWorkedWith'].str.split(';')\n",
    "\n",
    "# # Standardize technology names (strip extra spaces, title case)\n",
    "# df['Technologies'] = df['Technologies'].apply(lambda x: [tech.strip().title() for tech in x if tech])\n",
    "\n",
    "# # Flatten the list of technologies and create a set of unique technologies\n",
    "# all_technologies = set([tech for sublist in df['Technologies'] for tech in sublist])\n",
    "\n",
    "# # One-hot encode by creating a column for each technology\n",
    "# for tech in all_technologies:\n",
    "#     df[tech] = df['Technologies'].apply(lambda x: 1 if tech in x else 0)\n",
    "\n",
    "# # Drop the original 'LanguageHaveWorkedWith' and 'Technologies' columns\n",
    "# df.drop(columns=['LanguageHaveWorkedWith', 'Technologies'], inplace=True)\n",
    "\n",
    "# # Print the one-hot encoded results\n",
    "# # print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING DATABASE HAVE WORKED WITH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handle NaN values by filling them with an empty string or dropping rows with NaN values\n",
    "# df['DatabaseHaveWorkedWith'] = df['DatabaseHaveWorkedWith'].fillna('')\n",
    "\n",
    "# # Function to clean and one-hot encode a column\n",
    "# def clean_and_encode(column_name):\n",
    "#     # Split the column by semicolon\n",
    "#     df[column_name + '_Technologies'] = df[column_name].str.split(';')\n",
    "    \n",
    "#     # Standardize technology names (strip extra spaces, title case)\n",
    "#     df[column_name + '_Technologies'] = df[column_name + '_Technologies'].apply(lambda x: [tech.strip().title() for tech in x if tech])\n",
    "    \n",
    "#     # Flatten the list of technologies and create a set of unique technologies\n",
    "#     all_technologies = set([tech for sublist in df[column_name + '_Technologies'] for tech in sublist])\n",
    "    \n",
    "#     # One-hot encode by creating a column for each technology\n",
    "#     for tech in all_technologies:\n",
    "#         df[tech] = df[column_name + '_Technologies'].apply(lambda x: 1 if tech in x else 0)\n",
    "    \n",
    "#     # Drop the original technology columns\n",
    "#     df.drop(columns=[column_name, column_name + '_Technologies'], inplace=True)\n",
    "\n",
    "# # Clean and one-hot encode the 'DatabaseHaveWorkedWith' column\n",
    "# clean_and_encode('DatabaseHaveWorkedWith')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING DEVTYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace \"NA\" with NaN (missing values) in DevType column\n",
    "# df['DevType'] = df['DevType'].replace('NA', np.nan)\n",
    "\n",
    "# # Define a mapping to group similar roles\n",
    "# dev_type_mapping = {\n",
    "#     'Academic researcher': 'Researcher',\n",
    "#     'Blockchain': 'Developer',\n",
    "#     'Cloud infrastructure engineer': 'Engineer',\n",
    "#     'Data engineer': 'Data Professional',\n",
    "#     'Data or business analyst': 'Data Professional',\n",
    "#     'Data scientist or machine learning specialist': 'Data Professional',\n",
    "#     'Database administrator': 'Data Professional',\n",
    "#     'Designer': 'Designer',\n",
    "#     'Developer Advocate': 'Developer',\n",
    "#     'Developer Experience': 'Developer',\n",
    "#     'Developer, AI': 'Developer',\n",
    "#     'Developer back-end': 'Developer',\n",
    "#     'Developer, desktop or enterprise applications': 'Developer',\n",
    "#     'Developer, embedded applications': 'Developer',\n",
    "#     'Developer, front-end': 'Developer',\n",
    "#     'Developer, full-stack': 'Developer',\n",
    "#     'Developer, game or graphics': 'Developer',\n",
    "#     'Developer, mobile': 'Developer',\n",
    "#     'Developer, QA or test': 'Developer',\n",
    "#     'DevOps specialist': 'Engineer',\n",
    "#     'Educator': 'Educator',\n",
    "#     'Engineer site reliability': 'Engineer',\n",
    "#     'Engineering manager': 'Manager',\n",
    "#     'Hardware manager': 'Manager',\n",
    "#     'Marketing or sales professional': 'Business',\n",
    "#     'Others': 'Other',\n",
    "#     'Product manager': 'Manager',\n",
    "#     'Project manager': 'Manager',\n",
    "#     'Research and Development role': 'Researcher',\n",
    "#     'Scientist': 'Researcher',\n",
    "#     'Security professional': 'Security',\n",
    "#     'Senior Executive (C-Suite, VP)': 'Executive',\n",
    "#     'Student': 'Student',\n",
    "#     'System administrator': 'Engineer'\n",
    "# }\n",
    "\n",
    "# # Apply the mapping\n",
    "# df['DevType'] = df['DevType'].map(dev_type_mapping)\n",
    "\n",
    "# # Convert categorical column to numeric using a mapping for the grouped roles\n",
    "# dev_type_numeric_mapping = {\n",
    "#     'Researcher': 1,\n",
    "#     'Developer': 2,\n",
    "#     'Engineer': 3,\n",
    "#     'Data Professional': 4,\n",
    "#     'Designer': 5,\n",
    "#     'Manager': 6,\n",
    "#     'Business': 7,\n",
    "#     'Other': 8,\n",
    "#     'Educator': 9,\n",
    "#     'Security': 10,\n",
    "#     'Executive': 11,\n",
    "#     'Student': 12\n",
    "# }\n",
    "\n",
    "# # Apply the numeric mapping to the 'DevType' column\n",
    "# df['DevTypeNumeric'] = df['DevType'].map(dev_type_numeric_mapping)\n",
    "\n",
    "# # Fill NaN values (optional, depending on your analysis)\n",
    "# df['DevTypeNumeric'] = df['DevTypeNumeric'].fillna(-1)  # Use -1 or another placeholder for missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Analysis Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
